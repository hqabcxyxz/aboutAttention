{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Tutorial\n",
    "\n",
    "One of the most influential and interesting new neural networks types is the attention network. It's been used succesfully in translation services, [medical diagnosis](https://arxiv.org/pdf/1710.08312.pdf), and other tasks.\n",
    "\n",
    "Below we'll be walking through how to write your very own attention network. Our goal is to make a network that can translate human written times ('quarter after 3 pm') to military time ('15:15').\n",
    "\n",
    "The attention mechamism is defined in section **Model**.\n",
    "\n",
    "For a tutorial on how Attention Networks work, please visit [MuffinTech](http://muffintech.org/blog/id/12).\n",
    "\n",
    "Credit to Andrew Ng for reference model and inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:56.638099Z",
     "start_time": "2019-03-07T11:00:48.951917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Pinkie Pie was here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was created using some simple rules. It is not exhaustive, but provides some very nice challenges.\n",
    "\n",
    "The dataset is included in the Github repo.\n",
    "\n",
    "Some example data pairs are listed below:\n",
    "\n",
    "['48 min before 10 a.m', '09:12']  \n",
    "['t11:36', '11:36']  \n",
    "[\"nine o'clock forty six p.m\", '21:46']  \n",
    "['2:59p.m.', '14:59']  \n",
    "['23 min after 20 p.m.', '20:23']  \n",
    "['46 min after seven p.m.', '19:46']  \n",
    "['10 before nine pm', '20:50']  \n",
    "['3.20', '03:20']  \n",
    "['7.57', '07:57']  \n",
    "['six hours and fifty five am', '06:55']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:56.690695Z",
     "start_time": "2019-03-07T11:00:56.640942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'7': 10, 'k': 23, '4': 7, 'f': 19, 'v': 34, ' ': 0, 'o': 27, '2': 5, ':': 13, 't': 32, 'h': 21, 'y': 37, 'q': 29, 'p': 28, '<unk>': 39, 'a': 14, 'u': 33, '<pad>': 40, '.': 2, 'm': 25, 'g': 20, '5': 8, '9': 12, 'c': 16, 'i': 22, '0': 3, 'x': 36, 'r': 30, 'n': 26, 'l': 24, '1': 4, '8': 11, 'e': 18, 'b': 15, 's': 31, '6': 9, 'w': 35, \"'\": 1, 'z': 38, '3': 6, 'd': 17}\n",
      "{'6': 6, '7': 7, '4': 4, '5': 5, '1': 1, '9': 9, '2': 2, '0': 0, '8': 8, ':': 10, '3': 3}\n"
     ]
    }
   ],
   "source": [
    "with open('data/Time Dataset.json','r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "with open('data/Time Vocabs.json','r') as f:\n",
    "    human_vocab, machine_vocab = json.loads(f.read())\n",
    "    \n",
    "human_vocab_size = len(human_vocab)\n",
    "machine_vocab_size = len(machine_vocab)\n",
    "\n",
    "# Number of training examples\n",
    "m = len(dataset)\n",
    "print(m)\n",
    "# print(dataset[1])\n",
    "print(human_vocab)\n",
    "print(machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-07T07:31:48.616Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define some general helper methods. They are used to help tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:56.740814Z",
     "start_time": "2019-03-07T11:00:56.693593Z"
    },
    "code_folding": [
     0,
     41,
     63,
     71
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    A method for tokenizing data.\n",
    "    \n",
    "    Inputs:\n",
    "    dataset - A list of sentence data pairs.\n",
    "    human_vocab - A dictionary of tokens (char) to id's.\n",
    "    machine_vocab - A dictionary of tokens (char) to id's.\n",
    "    Tx - X data size\n",
    "    Ty - Y data size\n",
    "    \n",
    "    Outputs:\n",
    "    X - Sparse tokens for X data\n",
    "    Y - Sparse tokens for Y data\n",
    "    Xoh - One hot tokens for X data\n",
    "    Yoh - One hot tokens for Y data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metadata\n",
    "    m = len(dataset)\n",
    "    \n",
    "    # Initialize\n",
    "    X = np.zeros([m, Tx], dtype='int32')\n",
    "    Y = np.zeros([m, Ty], dtype='int32')\n",
    "    \n",
    "    # Process data\n",
    "    for i in range(m):\n",
    "        data = dataset[i]\n",
    "        X[i] = np.array(tokenize(data[0], human_vocab, Tx))\n",
    "        Y[i] = np.array(tokenize(data[1], machine_vocab, Ty))\n",
    "        if not i:\n",
    "            print(data)\n",
    "            print(\"X {}\".format(i),X[i])\n",
    "            print(\"Y {}\".format(i),Y[i])\n",
    "    \n",
    "    # Expand one hots\n",
    "    Xoh = oh_2d(X, len(human_vocab))\n",
    "    Yoh = oh_2d(Y, len(machine_vocab))\n",
    "    \n",
    "    return (X, Y, Xoh, Yoh)\n",
    "    \n",
    "def tokenize(sentence, vocab, length):\n",
    "    \"\"\"\n",
    "    Returns a series of id's for a given input token sequence.\n",
    "    \n",
    "    It is advised that the vocab supports <pad> and <unk>.\n",
    "    \n",
    "    Inputs:\n",
    "    sentence - Series of tokens\n",
    "    vocab - A dictionary from token to id\n",
    "    length - Max number of tokens to consider\n",
    "    \n",
    "    Outputs:\n",
    "    tokens - \n",
    "    \"\"\"\n",
    "    tokens = [0]*length\n",
    "    for i in range(length):\n",
    "        char = sentence[i] if i < len(sentence) else \"<pad>\"\n",
    "        char = char if (char in vocab) else \"<unk>\"\n",
    "        tokens[i] = vocab[char]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def ids_to_keys(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a series of id's into the keys of a dictionary.\n",
    "    \"\"\"\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    return [reverse_vocab[id] for id in sentence]\n",
    "\n",
    "def oh_2d(dense, max_value):\n",
    "    \"\"\"\n",
    "    Create a one hot array for the 2D input dense array.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    oh = np.zeros(np.append(dense.shape, [max_value]))\n",
    "    \n",
    "    # Set correct indices\n",
    "    ids1, ids2 = np.meshgrid(np.arange(dense.shape[0]), np.arange(dense.shape[1]))\n",
    "    \n",
    "    oh[ids1.flatten(), ids2.flatten(), dense.flatten('F').astype(int)] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal is to tokenize the data using our vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.113957Z",
     "start_time": "2019-03-07T11:00:56.743950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six hours and fifty five am', '06:55']\n",
      "X 0 [31 22 36  0 21 27 33 30 31  0 14 26 17  0 19 22 19 32 37  0 19 22 34 18\n",
      "  0 14 25 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40]\n",
      "Y 0 [ 0  6 10  5  5]\n",
      "(10000, 50)\n",
      "(10000, 5)\n",
      "(10000, 50, 41)\n",
      "(10000, 5, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 50 # Max x sequence length\n",
    "Ty = 5 # y sequence length\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Xoh.shape)\n",
    "print(Yoh.shape)\n",
    "\n",
    "# Split data 80-20 between training and test\n",
    "train_size = int(0.8*m)\n",
    "Xoh_train = Xoh[:train_size]\n",
    "Yoh_train = Yoh[:train_size]\n",
    "Xoh_test = Xoh[train_size:]\n",
    "Yoh_test = Yoh[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be careful, let's check that the code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.120953Z",
     "start_time": "2019-03-07T11:00:57.115351Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data point 4.\n",
      "\n",
      "The data input is: 8:25\n",
      "The data output is: 08:25\n",
      "\n",
      "The tokenized input is:[11 13  5  8 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40]\n",
      "The tokenized output is: [ 0  8 10  2  5]\n",
      "\n",
      "The one-hot input is: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "The one-hot output is: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "print(\"Input data point \" + str(i) + \".\")\n",
    "print(\"\")\n",
    "print(\"The data input is: \" + str(dataset[i][0]))\n",
    "print(\"The data output is: \" + str(dataset[i][1]))\n",
    "print(\"\")\n",
    "print(\"The tokenized input is:\" + str(X[i]))\n",
    "print(\"The tokenized output is: \" + str(Y[i]))\n",
    "print(\"\")\n",
    "print(\"The one-hot input is:\", Xoh[i])\n",
    "print(\"The one-hot output is:\", Yoh[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our next goal is to define our model. The important part will be defining the attention mechanism and then making sure to apply that correctly.\n",
    "\n",
    "Define some model metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.196846Z",
     "start_time": "2019-03-07T11:00:57.122054Z"
    }
   },
   "outputs": [],
   "source": [
    "layer1_size = 32\n",
    "layer2_size = 64 # Attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code snippets defined the attention mechanism. This is split into two arcs:\n",
    "\n",
    "* Calculating context\n",
    "* Creating an attention layer\n",
    "\n",
    "As a refresher, an attention network pays attention to certain parts of the input at each output time step. _attention_ denotes which inputs are most relevant to the current output step. An input step will have attention weight ~1 if it is relevant, and ~0 otherwise. The _context_ is the \"summary of the input\".\n",
    "\n",
    "The requirements are thus. The attention matrix should have shape $(T_x)$ and sum to 1. Additionally, the context should be calculated in the same manner for each time step. Beyond that, there is some flexibility. This notebook calculates both this way:\n",
    "\n",
    "$$\n",
    "attention = Softmax(Dense(Dense(x, y_{t-1})))\n",
    "$$\n",
    "<br/>\n",
    "$$\n",
    "context = \\sum_{i=1}^{m} ( attention_i * x_i )\n",
    "$$\n",
    "\n",
    "For safety, $y_0$ is defined as $\\vec{0}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.461755Z",
     "start_time": "2019-03-07T11:00:57.197910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define part of the attention layer gloablly so as to\n",
    "# share the same layers for each attention step.\n",
    "def softmax(x):\n",
    "    return K.softmax(x, axis=1)\n",
    "\n",
    "at_repeat = RepeatVector(Tx)\n",
    "at_concatenate = Concatenate(axis=-1)\n",
    "at_dense1 = Dense(8, activation=\"tanh\")\n",
    "at_dense2 = Dense(1, activation=\"relu\")\n",
    "at_softmax = Activation(softmax, name='attention_weights')\n",
    "at_dot = Dot(axes=1)\n",
    "\n",
    "def one_step_of_attention(h_prev, a):\n",
    "    \"\"\"\n",
    "    Get the context.\n",
    "    \n",
    "    Input:\n",
    "    h_prev - Previous hidden state of a RNN layer (m, n_h)\n",
    "    a - Input data, possibly processed (m, Tx, n_a)\n",
    "    \n",
    "    Output:\n",
    "    context - Current context (m, Tx, n_a)\n",
    "    \"\"\"\n",
    "    print(h_prev)\n",
    "    # Repeat vector to match a's dimensions\n",
    "    h_repeat = at_repeat(h_prev)\n",
    "    print(h_repeat)\n",
    "    # Calculate attention weights\n",
    "    i = at_concatenate([a, h_repeat])\n",
    "    i = at_dense1(i)\n",
    "    i = at_dense2(i)\n",
    "    attention = at_softmax(i)\n",
    "    # Calculate the context\n",
    "    context = at_dot([attention, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.475326Z",
     "start_time": "2019-03-07T11:00:57.467158Z"
    }
   },
   "outputs": [],
   "source": [
    "def attention_layer(X, n_h, Ty):\n",
    "    \"\"\"\n",
    "    Creates an attention layer.\n",
    "    Tx输入样本序列最大长度\n",
    "    Ty是标签最大长度w\n",
    "    \n",
    "    Input:\n",
    "    X - Layer input (m, Tx, x_vocab_size)\n",
    "    n_h - Size of LSTM hidden layer\n",
    "    Ty - Timesteps in output sequence\n",
    "    \n",
    "    Output:\n",
    "    output - The output of the attention layer (m, Tx, n_h)\n",
    "    \"\"\"    \n",
    "    # Define the default state for the LSTM layer\n",
    "    h = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)),name='LAMBDA1')(X)\n",
    "    c = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)),name='LAMBDA2')(X)\n",
    "    # Messy, but the alternative is using more Input()\n",
    "    \n",
    "    at_LSTM = LSTM(n_h, return_state=True)\n",
    "    \n",
    "    output = []\n",
    "              \n",
    "    # Run attention step and RNN for each output time step\n",
    "    for _ in range(Ty):\n",
    "        context = one_step_of_attention(h, X)\n",
    "        \n",
    "        h, _, c = at_LSTM(context, initial_state=[h, c])\n",
    "        \n",
    "        output.append(h)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample model is organized as follows:\n",
    "\n",
    "1. BiLSTM\n",
    "2. Attention Layer\n",
    "    * Outputs Ty lists of activations.\n",
    "3. Dense\n",
    "    * Necessary to convert attention layer's output to the correct y dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里注意Tx是50,x_vocab_size是41  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:57.567626Z",
     "start_time": "2019-03-07T11:00:57.476355Z"
    }
   },
   "outputs": [],
   "source": [
    "layer3 = Dense(machine_vocab_size, activation=softmax)\n",
    "\n",
    "def get_model(Tx, Ty, layer1_size, layer2_size, x_vocab_size, y_vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a model.\n",
    "    \n",
    "    input:\n",
    "    Tx - Number of x timesteps\n",
    "    Ty - Number of y timesteps\n",
    "    size_layer1 - Number of neurons in BiLSTM\n",
    "    size_layer2 - Number of neurons in attention LSTM hidden layer\n",
    "    x_vocab_size - Number of possible token types for x\n",
    "    y_vocab_size - Number of possible token types for y\n",
    "    \n",
    "    Output:\n",
    "    model - A Keras Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create layers one by one\n",
    "    X = Input(shape=(Tx, x_vocab_size))\n",
    "    \n",
    "    a1 = Bidirectional(LSTM(layer1_size, return_sequences=True), merge_mode='concat')(X)\n",
    "    \n",
    "\n",
    "    a2 = attention_layer(a1, layer2_size, Ty)\n",
    "    \n",
    "    a3 = [layer3(timestep) for timestep in a2]\n",
    "        \n",
    "    # Create Keras model\n",
    "    model = Model(inputs=[X], outputs=a3)\n",
    "    \n",
    "    print(len(a2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps from here on out are for creating the model and training it. Simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_model（50，5，32，64，41，11）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:59.301945Z",
     "start_time": "2019-03-07T11:00:57.573722Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LAMBDA1/zeros:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"repeat_vector_1/Tile:0\", shape=(?, 50, 64), dtype=float32)\n",
      "Tensor(\"lstm_2/TensorArrayReadV3:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"repeat_vector_1_1/Tile:0\", shape=(?, 50, 64), dtype=float32)\n",
      "Tensor(\"lstm_2_1/TensorArrayReadV3:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"repeat_vector_1_2/Tile:0\", shape=(?, 50, 64), dtype=float32)\n",
      "Tensor(\"lstm_2_2/TensorArrayReadV3:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"repeat_vector_1_3/Tile:0\", shape=(?, 50, 64), dtype=float32)\n",
      "Tensor(\"lstm_2_3/TensorArrayReadV3:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"repeat_vector_1_4/Tile:0\", shape=(?, 50, 64), dtype=float32)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtain a model instance\n",
    "model = get_model(Tx, Ty, layer1_size, layer2_size, human_vocab_size, machine_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:59.415211Z",
     "start_time": "2019-03-07T11:00:59.303027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50, 41)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 64)       18944       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "LAMBDA1 (Lambda)                (None, 64)           0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 50, 64)       0           LAMBDA1[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50, 8)        1032        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50, 1)        9           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 50, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "LAMBDA2 (Lambda)                (None, 64)           0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 LAMBDA1[0][0]                    \n",
      "                                                                 LAMBDA2[0][0]                    \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[3][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "==================================================================================================\n",
      "Total params: 53,724\n",
      "Trainable params: 53,724\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiebotgpuhq/anaconda3/envs/kerasEnv/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'LAMBDA1/zeros:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'LAMBDA2/zeros:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/chiebotgpuhq/anaconda3/envs/kerasEnv/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/chiebotgpuhq/anaconda3/envs/kerasEnv/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/chiebotgpuhq/anaconda3/envs/kerasEnv/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/chiebotgpuhq/anaconda3/envs/kerasEnv/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "opt = Adam(lr=0.05, decay=0.04, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "json_string = model.to_json()\n",
    "with open('attention.json','w') as fw:\n",
    "    fw.write(json_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AttentionKeras](https://github.com/hqabcxyxz/MarkDownPics/blob/master/%E5%85%B6%E4%BB%96%E5%9C%B0%E6%96%B9%E5%9B%BE%E7%89%87/AttentionKeras.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:00:59.418013Z",
     "start_time": "2019-03-07T11:00:59.416388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group the output by timestep, not example\n",
    "outputs_train = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:01:17.457911Z",
     "start_time": "2019-03-07T11:00:59.418927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will start train\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 6.8280 - dense_3_loss: 1.7234 - dense_3_acc: 0.5004 - dense_3_acc_1: 0.2123 - dense_3_acc_2: 0.9499 - dense_3_acc_3: 0.4024 - dense_3_acc_4: 0.3849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe682cae630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time to train\n",
    "# It takes a few minutes on an quad-core CPU\n",
    "print(\"I will start train\")\n",
    "model.fit([Xoh_train], outputs_train, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The final training loss should be in the range of 0.02 to 0.5\n",
    "\n",
    "The test loss should be at a similar level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:01:19.493081Z",
     "start_time": "2019-03-07T11:01:17.458969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2s 1ms/step\n",
      "Test loss:  4.120207370758057\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test performance\n",
    "outputs_test = list(Yoh_test.swapaxes(0,1))\n",
    "score = model.evaluate(Xoh_test, outputs_test) \n",
    "print('Test loss: ', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created this beautiful model, let's see how it does in action.\n",
    "\n",
    "The below code finds a random example and runs it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:01:19.869302Z",
     "start_time": "2019-03-07T11:01:19.494114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: twenty two min before nine a.m.\n",
      "Tokenized: [32 35 18 26 32 37  0 32 35 27  0 25 22 26  0 15 18 19 27 30 18  0 26 22\n",
      " 26 18  0 14  2 25  2 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40]\n",
      "Prediction: [1, 1, 10, 2, 2]\n",
      "Prediction text: 11:22\n"
     ]
    }
   ],
   "source": [
    "# Let's visually check model output.\n",
    "import random as random\n",
    "\n",
    "i = random.randint(0, m)\n",
    "\n",
    "def get_prediction(model, x):\n",
    "    prediction = model.predict(x)\n",
    "    max_prediction = [y.argmax() for y in prediction]\n",
    "    str_prediction = \"\".join(ids_to_keys(max_prediction, machine_vocab))\n",
    "    return (max_prediction, str_prediction)\n",
    "\n",
    "max_prediction, str_prediction = get_prediction(model, Xoh[i:i+1])\n",
    "\n",
    "print(\"Input: \" + str(dataset[i][0]))\n",
    "print(\"Tokenized: \" + str(X[i]))\n",
    "print(\"Prediction: \" + str(max_prediction))\n",
    "print(\"Prediction text: \" + str(str_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, all introductions to Attention networks require a little tour.\n",
    "\n",
    "The below graph shows what inputs the model was focusing on when writing each individual letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T11:01:20.508759Z",
     "start_time": "2019-03-07T11:01:19.870418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x129.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = random.randint(0, m)\n",
    "\n",
    "def plot_attention_graph(model, x, Tx, Ty, human_vocab, layer=7):\n",
    "    # Process input\n",
    "    tokens = np.array([tokenize(x, human_vocab, Tx)])\n",
    "    tokens_oh = oh_2d(tokens, len(human_vocab))\n",
    "    \n",
    "    # Monitor model layer\n",
    "    layer = model.layers[layer]\n",
    "    \n",
    "    layer_over_time = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    layer_output = layer_over_time([tokens_oh])\n",
    "    layer_output = [row.flatten().tolist() for row in layer_output]\n",
    "    \n",
    "    # Get model output\n",
    "    prediction = get_prediction(model, tokens_oh)[1]\n",
    "    \n",
    "    # Graph the data\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(1.8)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    plt.title(\"Attention Values per Timestep\")\n",
    "    \n",
    "    plt.rc('figure')\n",
    "    cax = plt.imshow(layer_output, vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    plt.xlabel(\"Input\")\n",
    "    ax.set_xticks(range(Tx))\n",
    "    ax.set_xticklabels(x)\n",
    "    \n",
    "    plt.ylabel(\"Output\")\n",
    "    ax.set_yticks(range(Ty))\n",
    "    ax.set_yticklabels(prediction)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_attention_graph(model, dataset[i][0], Tx, Ty, human_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kerasEnv]",
   "language": "python",
   "name": "conda-env-kerasEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "255px",
    "width": "248px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
